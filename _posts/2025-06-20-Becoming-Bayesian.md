---
layout: post
title: Becoming Bayesian in HCI & USEC - The when, how and why.
categories: blog
tags: [statistics, bayes, frequentist, research, USEC, HCI]
published: false
---

Hi everyone. I have, since about 2 years, been working on educating myself (alongside being educated by others) in Bayesian statistics. I love working with data and statistics, so to me this was just a matter of time. And boy, did it pay off. "What's Bayesian Statistics?," you might ask. Well, that's partially what this blog post is about. I will tell you what prompted me to learn about it, why it is useful, and why you should consider either switching to bayesian stats or start learning it, in case you're new to the world of quantitative research. I will start with a basic explanation of the existing statistical approaches, their differences and their goals, then will tell you about the differences, and outline why the Bayesian version can give you an edge. Finally, I will outline some recommendations and propose a workflow that you should definitely consider, if you plan on working with data and bayesian approaches. So let's dive right into the basics. 

The statistical landscape 

Statistics is basically divided in two philosophical schools. You might wonder "Philosophy? I thought statistics was about maths!". This, in my opinion, is only partially correct. I personally consider statistics, especially applied statistics, mostly an area of philosophy with some maths attached. This is because the part of statistics most applications deal with has very little to do with maths and very much to do with interpretation and context. And this is what those two mentioned schools of statistics are about: 
The most common school, frequentism, has a very specific view on what probability represents. In frequentism, any probability estimate or statement is a long-run limit. If a frequentist says the probability of your toast forcefully evacuating the toaster is 70%, it means that if you observe an inifinite amount or very large number of toasts, 70% would shoot out of the toaster like rockets. But you cannot predict the probability of what an individual toast will do. 
Bayesianism, the second school, solves this issue. They see probability as somewhat of a subjective estimate that is dependent on prior knowledgde and data. If you know that your toaster model by default has a rate of at least 50% forceful evacuations and your specific apparatus at home has shown 60% rocket toasties so far, you can calculate the probability with which your toaster will play launchpad for the next toast.

Whats the big deal?

If you are reading this post, chances are you probably have either encountered statistics in the frequentist way in reading or have used it. Frequentist statistics are at time of writing the de-facto standard. However, as you might have been able to tell from the example I gave earlier, there are a few limitations to this approach. Most commonly criticised about frequentist statistics is the use of one of its hallmarks, the p-value. I will tell you what the p-value is because most people either learn it wrongly or never understand it. A p-value is used to evaluate whether a hypothesis (the so-called null-hypothesis) can be rejected or not. To do this, a researcher formulates an alternative hypothesis, like "on average, the toasts will evacuate with speed more than half the time". This is being put to the test by probing the opposite, the null-hypothesis. The null says that the toaster will evacuate forcefully less than half the time. We then gather data and see what we have, use an estimation function to generate a p-value and check whether our sample could be generalized to all toasters. If the found effect (let's say more than 50% are forceful toasties) has a p-value below a threshold (this could be anything, as it is researcher-specified) we accept that our null hypothesis is probably false. The p-value we gathered is a probability that our current data would arise in a world in which our null-hypothesis was true. It is a conditional probability. And it has nothing to do with the thing we want to know, apart from being the opposite - which does not tell us much. I hope this is clear.
Another big issue is the fact that frequentism requires a lot of data to solve complex issues. This is a serious constraint, because most things are highly complex and therefore need a lot of data to get right. And data is expensive. Many researchers just cannot afford gathering the amount of data they actually need for their project. TBC

This means its methods perform exceptionally in such environments, but come with constraints. These are often model assumptions that dictate in which environments the procedures work and in which they don't. Unfortunately, we are discovering that more and more areas in which we thought they would work actually do not permit their use. 

Why is frequentism so prominent then?

As I see it, there are two reasons. Firstly, frequentism emerged in the 19th century and was made for hand calculations or low-end computation. This situation was only changing after 1990 with the increasing bandwidth of computers. So until then, researchers learnt frequentist methods because those were the best tools for the job in the environment they had. And they kept passing on those methods to their students. Obviously, there is a tradition involved, and many students still learn these methods from their teachers who themselves learnt them in the 90s or before. 

The second reason is that frequentist methods are less complex than Bayesian methods. You can run and understand a t-test with little work. Other common models are similar. You (seemingly) don't need to learn much to read and understand them, and application is also not that hard, especially with point-and-click software like SPSS. The internally twisted logic is also missed by most by virtue of factually wrong teaching practices. 

Both reasons together really make clear that there is no real reason why we should not look further. Computation has dramatically improved since, and scientists are increasingly eager to improve their inferences and make use of that powerful machine. 