---
published: false
---

# Building Software to Make Phishing Research Meaningful

Hey, friend. Since you're reading this blog, you're probably interested in phishing. You know, the thing that more than 90% of cyber attacks use to gain access to systems. The bad emails containing links your IT department doesn't want you to click. Or the malicious excel files that download malware on your computer (ok, all excel-files are somewhat malicious, but these ones especially so). We all know, *phishing is a problem*. But what many outside the phishing research community don't know: *phishing research also is a problem*. I recognized this pretty early during my master's thesis for which I did some phishing email research, so my first project of my PhD was trying to come up with a solution to the problem. This blog post is about the product and the paper that we wrote about it. But first things first. In order to understand what this all is about, you need to be aware of the subject first. 

### The Problem

Let me give you the quick and dirty history of phishing research. Phishing is now a quarter decade old. Academic research about phishing started in the early 2000s, when people found the phenomenon to become more pervasive, and the internet was adopted more heavily. Soon enough, researchers started sending out phishing emails to (at the time involuntary) participants themselves to see whether they can gain some knowledge from that. Turns out, they couldn't get much. Yes, some emails with specific wording or social cues were clicked more often - and that's where people still draw advice from ("if they say it's urgent, be suspicious"), but other than that such research can not tell you much about other factors influencing the decision to click on a link in a phishing email. Personality traits, external circumstances or even specific elements in phishing emails could not be researched with such large campaign studies. And of course, this is important. We need to know what it is that makes users click on seemingly awfully constructed fake emails. Of course, the spear-phishing ones are also interesting, but those often are hard to combat. I would argue: To maximize research impact, it is best to know what makes most people click on the emails - and those cases are usually run-of-the-mill types of phishing.

To sort-of do this, researchers then adopted what is called scenario-based (or role-play) study design. It usually entails that a participant in a laboratory setting gets to either view an image of an email or a locked-down inbox with an email, immerse themselves in the situation of a made-up office or work environment, and has to decide whether it is phishing or not. In some cases they have to evaluate it with some different trust measure. While this enables you to indeed inquire about other constructs or effects influencing phishing detection, they have some problems of themselves. And to be honest, they're not hard to spot. Take a second and think about potential pitfalls with such designs. 

Yes, you guessed correctly. It's the environment. It's weird that we put participants into an isolated room and have them pretend it's any other tuesday. This problem is called a threat to ecological validity, and describes that it's really hard to extrapolate such lab studies to real life. Related to this is the fact that researchers often instruct participants to classify the emails into phishing emails and real ones, or rate them in terms of trust or some similar measure. This is a classic example of a research design that can lead to enforced vigilance - something that does not happen outside in the "real world". And it biases your study design. Maybe just telling them what to look for or what to rate makes your participants more suspicious than they would be in the office or at home. All of this --- you guessed it --- makes the results of such studies really hard to generalize to real life situations, which happen often, and with a lot of noise (or variation). Also imagine having to build a study design that really makes sure the participants believe in the situation they're supposedly in. You'd have to implement a lot of checks and balances to make sure that the things you're measuring are what you're intending to (this is called internal validity in methodological lingo). The reason why this is important, is the goal I mentioned earlier: Research wants to be applicable to real problems - ideally. "But these scenarios are the best we have so far", the researchers thought, and, so far, nobody thought of improving the situation. So, my colleagues and I thought about different solutions to this problem, and we stumbled upon a conundrum: The phishing campaign studies were always simulating the problem, being the emails. But this approach in the same manner limited their ability to gain insight into the problem. It's like wanting to cut the hair at the back of your head by yourself while also observing results. Kind of impossible. Unless you put up a big stack of mirrors. And that's what we did.

### PhishyMailbox

We simulated the environment instead of the problem, or, in other words: We built an app. This app is called *PhishyMailbox*. It looks and feels like a generic webmail client, one you could find for example with Outlook - just a bit different, for patent reasons. It is a web app (i.e. running in the browser) built for researchers and practitioners and facilitates study design. It allows you to insert emails into a simulated inbox. These emails then can be viewed and interacted with by users just like you could view and interact with emails in your inbox, with the exception that you can't answer them. Basically, we designed it to be used with a mail sorting task, a paradigm known for half a century from organizational and managerial science. This exercise gives participants a stack of mail (nowadays: email) and lets them sort it by priority. This can be adapted to different roles or contexts and is normally used to assess task management skills. Some phishing research has already used this task with the methodology described above to let participants sort "mail", while adding phishing emails in the stack to see how participants react to them. Unfortunately, these studies were limited in their ability to measure behavioral data and had to observe participants - which automatically limits the sample size, as you have to do it in a lab. This is where PhishyMailbox comes in. 

### Why researchers would like this

The (scientific) potential of phishy mailbox lies in the details we developed for researchers. The measurement is done by the program. We record a plethora of events enacted by the participants, none of which are personally identifyable data. Since this also does not rely on self report, we can actually measure behavior with this app --- which is rare in my field. Additionally, we don't have to be there to watch participants, making them feel more at home. In fact, they can actually be at home while taking part in the study, as the program allows deployment on a server and sending out links for remote participation. We added customizable intro and outro messages, time restriction functions with accompanying UIs and the possibility to chain other web links, surveys, tasks, consent forms or anything else you could link to to the study --- before start or after completion. Participant tokens can also be passed to other sites after study participation via link parameters so you can connect demographic data from a survey with participant IDs, for example. This means: You can use PhishyMailbox in almost every workflow imaginable. 

### Evaluation

Of course, since we're academics here, we had to evaluate the app. We did so with scientists from the usable security field. We invited 5 of them to come and use the program. For this session, we prepared 6 tasks that represent typical workflows with PhishyMailbox and asked participants to perform them without any prior experience other than a basic explanation of the program and a look at the user interface. 


### User Journey

### Further Development


### Resources

1. Paper: [Link to Repository](https://)
2. GitHub Repo: [PhishyMailbox](https://github.com/Enterprize1/phishy-mailbox)
3. Dockerhub Image: [Dockerhub](https://hub.docker.com/r/thorstenthiel/phishy-mailbox)
